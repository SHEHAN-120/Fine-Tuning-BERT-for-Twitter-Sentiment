# -*- coding: utf-8 -*-
"""sentiment_classifier.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Pb7RlIYXF6hGMrHgi4r00AMzS4-pCD4v

## Install Libiraries
"""

!pip install -U transformers
!pip install -U accelerate
!pip install -U datasets
!pip install -U bertviz
!pip install -U umap-learn
!pip install seaborn --upgrade

import pandas as pd

df = pd.read_csv("https://raw.githubusercontent.com/laxmimerit/All-CSV-ML-Data-Files-Download/master/twitter_multi_class_sentiment.csv")

df.head()

df.info()
df.isnull().sum()

df['label'].value_counts()

"""## Dataset Analysics"""

import matplotlib.pyplot as plt

label_counts=df['label_name'].value_counts(ascending=True)
label_counts.plot.barh()
plt.title("Frequency of Classes")
plt.show()

df['words per tweet']=df['text'].str.split().apply(len)
df.boxplot("words per tweet",by="label_name")

"""## Text to Token Conversion"""

from transformers import AutoTokenizer

model_ckpt="bert-base-uncased"
tokenizer=AutoTokenizer.from_pretrained(model_ckpt)

text="I Love machine learning! Tokenization is awesome!!"
encoded_text=tokenizer(text)
print(encoded_text)

len(tokenizer.vocab),tokenizer.vocab_size,tokenizer.model_max_length

"""## Data Loader and Train Test Split"""

from sklearn.model_selection import train_test_split

train,test=train_test_split(df,test_size=0.3,stratify=df['label_name'])
test,validation=train_test_split(test,test_size=1/3,stratify=test['label_name'])
train.shape, test.shape, validation.shape

from datasets import Dataset,DatasetDict

dataset=DatasetDict(
    {'train':Dataset.from_pandas(train,preserve_index=False),
     'test':Dataset.from_pandas(test,preserve_index=False),
     'validation':Dataset.from_pandas(validation,preserve_index=False)
     }
)

dataset

"""## Tokenization of the Sentiment Data"""

dataset['train'][0],dataset['train'][1]

def tokenize(batch):
  temp=tokenizer(batch['text'],padding=True,truncation=True)
  return temp

print(tokenize(dataset['train'][:2]))

emotion_encoded=dataset.map(tokenize,batched=True,batch_size=None)

label2id={x['label_name']:x['label'] for x in dataset['train']}
id2label={v:k for k,v in label2id.items()}

label2id,id2label

"""## Model Building"""

from transformers import AutoModel
import torch

model=AutoModel.from_pretrained(model_ckpt)

model.config

"""## Fine Tuning Transformers"""

from transformers import AutoModelForSequenceClassification,AutoConfig

num_labels=len(label2id)
device=torch.device("cuda" if torch.cuda.is_available() else "cpu")
config=AutoConfig.from_pretrained(model_ckpt,label2id=label2id,id2label=id2label)
model=AutoModelForSequenceClassification.from_pretrained(model_ckpt,config=config).to(device)

model.config

from transformers import TrainingArguments

batch_size=64
training_dir="bert_base_train_dir"

training_args=TrainingArguments( output_dir=training_dir,
                                 overwrite_output_dir=True,
                                 num_train_epochs=2,
                                 learning_rate=2e-5,
                                 per_device_train_batch_size=batch_size,
                                 per_device_eval_batch_size=batch_size,
                                 weight_decay=0.01,
                                 eval_strategy='epoch',
                                 report_to="none",
                                 disable_tqdm=False
                                 )

from sklearn.metrics import accuracy_score,f1_score

def compute_metrics(pred):
  labels=pred.label_ids
  preds=pred.predictions.argmax(-1)

  f1=f1_score(labels,preds,average="weighted")
  acc=accuracy_score(labels,preds)

  return {"accuracy": acc, "f1":f1}

"""## Build the Trainer"""

from transformers import Trainer

trainer=Trainer(model=model,args=training_args,
                compute_metrics=compute_metrics,
                train_dataset=emotion_encoded['train'],
                eval_dataset=emotion_encoded['validation'],
                tokenizer=tokenizer)

import os
os.environ["WANDB_DISABLED"] = "true"

trainer.train()

"""## Model Evaluation"""

preds_output=trainer.predict(emotion_encoded['test'])
preds_output.metrics

import numpy as np

y_pred=np.argmax(preds_output.predictions,axis=1)
y_true=emotion_encoded['test'][:]['label']

from sklearn.metrics import classification_report
print(classification_report(y_true,y_pred))

import seaborn as sns
from sklearn.metrics import confusion_matrix
import matplotlib.pyplot as plt

cm=confusion_matrix(y_true,y_pred)

plt.figure(figsize=(5,5))
sns.heatmap(cm,annot=True,xticklabels=label2id.keys(),yticklabels=label2id.keys(),fmt="d",cbar=False,cmap="Blues")
plt.ylabel("Actual")
plt.xlabel("Predicted")
plt.show()

"""## Build the Predictions and Save the Model"""

text="I Love that Comment"

def get_prediction(text):
  input_encoded=tokenizer(text,return_tensors="pt").to(device)

  with torch.no_grad():
    outputs=model(**input_encoded)

  logits=outputs.logits

  pred=torch.argmax(logits,dim=1).item()
  return id2label[pred]

get_prediction(text)

trainer.save_model("bert-base-uncased-sentiment-model")

"""## Used Pipeline for Predictions"""

from transformers import pipeline

text="Wow! .It was a amazing journey"

classifier=pipeline('text-classification',model='/content/bert-base-uncased-sentiment-model')

classifier(text)